Model,Assumptions,Preprocessing Requirements,Violations,Considerations for Imbalanced Data,Multicollinearity,Handling Categorical Data
Logistic Regression,Linearity between independent variables and log-odds of the dependent variable; No multicollinearity; Variables should be relevant to the problem.,Check for multicollinearity; One-hot encoding for categorical variables; Remove or impute missing values.,Imbalanced data can lead to biased results; Outliers can have significant effects; Violates the assumption of linearity with non-linear data.,"Use class weights or resampling techniques (SMOTE, undersampling); Performance metrics like AUC-ROC, Precision-Recall are more informative than accuracy.","Must remove or reduce multicollinearity (e.g., through VIF, PCA).",One-hot encoding required for categorical variables.
XGBoost,No assumptions about the data distribution; Works with both continuous and categorical variables; Handles non-linearity well.,"One-hot encoding or target encoding for categorical variables; Missing values are handled natively, but imputation can improve performance.","Sensitive to high cardinality in categorical variables; Can overfit on small, imbalanced datasets without proper tuning.","Tune parameters (scale_pos_weight, learning rate) to handle imbalance; Use AUC-ROC, Precision-Recall for evaluation.",Not sensitive to multicollinearity due to the nature of boosting.,One-hot encoding or target encoding required for categorical variables.
LightGBM,No assumptions about data distribution; Handles both continuous and categorical variables; Assumes independence between trees.,"One-hot or target encoding for categorical variables; Missing values can be handled internally, but imputation is recommended.",Sensitive to noisy data; Imbalanced datasets may cause performance issues; May overfit without regularization.,"Similar considerations as XGBoost; Use class_weight or oversampling techniques; Regularization (lambda, alpha) to handle overfitting.","Not highly sensitive to multicollinearity, though removing highly correlated features may improve performance.",One-hot encoding or target encoding required for categorical variables.
Random Forest,No assumptions about data distribution; Handles continuous and categorical variables; Assumes independence between trees.,One-hot encoding for categorical variables; Handle missing data by imputation or allow the model to handle missing splits.,Imbalanced data may lead to biased results; Can overfit on small or noisy datasets without pruning.,"Use class_weight or resampling techniques; Evaluate with AUC-ROC, Precision-Recall; Pruning may be needed to avoid overfitting.","Not sensitive to multicollinearity due to decision tree nature, but removing redundant features may help.",One-hot encoding required for categorical variables.
