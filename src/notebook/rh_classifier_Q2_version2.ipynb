{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation and Handling\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "\n",
    "# DB Credentials\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Handling Imbalanced Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Model Lifecycle Management\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Distributed Computing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier as SparkRFClassifier\n",
    "\n",
    "# Model Interpretability\n",
    "import shap\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Automated Feature Engineering\n",
    "import featuretools as ft\n",
    "\n",
    "# Add parent directory to sys.path\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Custom Modules\n",
    "from fetch_data_hook import fetch_sql_code, fetch_sql_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. EDA Analysis\n",
    "# 2. Feature Engineering\n",
    "# 3. Train-Test Split\n",
    "# 4. Feature Scaling: {normalization/standardization, dimension reduction techniques, handling imbalance datasets/sampling}\n",
    "# 5. Model Training -> Tuning -> Evaluating\n",
    "# 6. Model Prediction\n",
    "# 7. Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>risk_tolerance</th>\n",
       "      <th>investment_experience</th>\n",
       "      <th>liquidity_needs</th>\n",
       "      <th>platform</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>instrument_type_first_traded</th>\n",
       "      <th>first_deposit_amount</th>\n",
       "      <th>time_horizon</th>\n",
       "      <th>user_id</th>\n",
       "      <th>churn_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>33.129417</td>\n",
       "      <td>stock</td>\n",
       "      <td>40.0</td>\n",
       "      <td>med_time_horizon</td>\n",
       "      <td>895044c23edc821881e87da749c01034</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>16.573517</td>\n",
       "      <td>stock</td>\n",
       "      <td>200.0</td>\n",
       "      <td>short_time_horizon</td>\n",
       "      <td>458b1d95441ced242949deefe8e4b638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>iOS</td>\n",
       "      <td>10.008367</td>\n",
       "      <td>stock</td>\n",
       "      <td>25.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>c7936f653d293479e034865db9bb932f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>1.031633</td>\n",
       "      <td>stock</td>\n",
       "      <td>100.0</td>\n",
       "      <td>short_time_horizon</td>\n",
       "      <td>b255d4bd6c9ba194d3a350b3e76c6393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>8.187250</td>\n",
       "      <td>stock</td>\n",
       "      <td>20.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>4a168225e89375b8de605cbc0977ae91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579</th>\n",
       "      <td>high_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>8.339283</td>\n",
       "      <td>stock</td>\n",
       "      <td>300.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>03880c726d8a4e5db006afe4119ad974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5580</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>somewhat_important_liq_need</td>\n",
       "      <td>iOS</td>\n",
       "      <td>7.241383</td>\n",
       "      <td>stock</td>\n",
       "      <td>100.0</td>\n",
       "      <td>short_time_horizon</td>\n",
       "      <td>ae8315109657f44852b24c6bca4decd6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5581</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>no_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>both</td>\n",
       "      <td>22.967167</td>\n",
       "      <td>stock</td>\n",
       "      <td>50.0</td>\n",
       "      <td>short_time_horizon</td>\n",
       "      <td>f29c174989f9737058fe808fcf264135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5582</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>somewhat_important_liq_need</td>\n",
       "      <td>iOS</td>\n",
       "      <td>10.338417</td>\n",
       "      <td>stock</td>\n",
       "      <td>100.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>24843497d1de88b2e7233f694436cb3a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5583</th>\n",
       "      <td>high_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>somewhat_important_liq_need</td>\n",
       "      <td>iOS</td>\n",
       "      <td>18.470950</td>\n",
       "      <td>stock</td>\n",
       "      <td>50.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>49ee0531ee9dfbce0e7d9afa1c3d86f4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5584 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           risk_tolerance   investment_experience  \\\n",
       "0     high_risk_tolerance  limited_investment_exp   \n",
       "1      med_risk_tolerance  limited_investment_exp   \n",
       "2      med_risk_tolerance  limited_investment_exp   \n",
       "3      med_risk_tolerance  limited_investment_exp   \n",
       "4     high_risk_tolerance  limited_investment_exp   \n",
       "...                   ...                     ...   \n",
       "5579  high_risk_tolerance  limited_investment_exp   \n",
       "5580   med_risk_tolerance  limited_investment_exp   \n",
       "5581   med_risk_tolerance       no_investment_exp   \n",
       "5582   med_risk_tolerance  limited_investment_exp   \n",
       "5583  high_risk_tolerance  limited_investment_exp   \n",
       "\n",
       "                  liquidity_needs platform  time_spent  \\\n",
       "0         very_important_liq_need  Android   33.129417   \n",
       "1         very_important_liq_need  Android   16.573517   \n",
       "2         very_important_liq_need      iOS   10.008367   \n",
       "3         very_important_liq_need  Android    1.031633   \n",
       "4         very_important_liq_need  Android    8.187250   \n",
       "...                           ...      ...         ...   \n",
       "5579      very_important_liq_need  Android    8.339283   \n",
       "5580  somewhat_important_liq_need      iOS    7.241383   \n",
       "5581      very_important_liq_need     both   22.967167   \n",
       "5582  somewhat_important_liq_need      iOS   10.338417   \n",
       "5583  somewhat_important_liq_need      iOS   18.470950   \n",
       "\n",
       "     instrument_type_first_traded  first_deposit_amount        time_horizon  \\\n",
       "0                           stock                  40.0    med_time_horizon   \n",
       "1                           stock                 200.0  short_time_horizon   \n",
       "2                           stock                  25.0   long_time_horizon   \n",
       "3                           stock                 100.0  short_time_horizon   \n",
       "4                           stock                  20.0   long_time_horizon   \n",
       "...                           ...                   ...                 ...   \n",
       "5579                        stock                 300.0   long_time_horizon   \n",
       "5580                        stock                 100.0  short_time_horizon   \n",
       "5581                        stock                  50.0  short_time_horizon   \n",
       "5582                        stock                 100.0   long_time_horizon   \n",
       "5583                        stock                  50.0   long_time_horizon   \n",
       "\n",
       "                               user_id  churn_flag  \n",
       "0     895044c23edc821881e87da749c01034           0  \n",
       "1     458b1d95441ced242949deefe8e4b638           0  \n",
       "2     c7936f653d293479e034865db9bb932f           0  \n",
       "3     b255d4bd6c9ba194d3a350b3e76c6393           0  \n",
       "4     4a168225e89375b8de605cbc0977ae91           0  \n",
       "...                                ...         ...  \n",
       "5579  03880c726d8a4e5db006afe4119ad974           0  \n",
       "5580  ae8315109657f44852b24c6bca4decd6           1  \n",
       "5581  f29c174989f9737058fe808fcf264135           0  \n",
       "5582  24843497d1de88b2e7233f694436cb3a           0  \n",
       "5583  49ee0531ee9dfbce0e7d9afa1c3d86f4           0  \n",
       "\n",
       "[5584 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df = fetch_sql_code('''\n",
    "WITH temp1 AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) AS rn,\n",
    "        timestamp::date - ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp)::int AS streak_id\n",
    "    FROM\n",
    "        equity_value_data\n",
    "),\n",
    "temp2 AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        MIN(timestamp::date) AS start_streak_date,\n",
    "        MAX(timestamp::date) AS end_streak_date,\n",
    "        COUNT(*) AS duration_of_above10_streak\n",
    "    FROM\n",
    "        temp1\n",
    "    GROUP BY\n",
    "        user_id, streak_id\n",
    "),\n",
    "temp3 AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        LAG(end_streak_date) OVER (PARTITION BY user_id ORDER BY start_streak_date ASC) AS prev_above10_streak_date,\n",
    "        start_streak_date - LAG(end_streak_date) OVER (PARTITION BY user_id ORDER BY start_streak_date ASC) AS duration_between_above10_streaks\n",
    "    FROM\n",
    "        temp2\n",
    ")\n",
    "SELECT distinct user_id\n",
    "FROM temp3\n",
    "WHERE duration_between_above10_streaks >= 28\n",
    "''')\n",
    "churn_df\n",
    "churn_users = set(churn_df['user_id'].tolist())\n",
    "\n",
    "df = fetch_sql_code('''\n",
    "select * from features_data\n",
    "''')\n",
    "\n",
    "df['churn_flag'] = df['user_id'].apply(lambda x: 1 if x in churn_users else 0 )\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This pipeline includes the following:\n",
    "\n",
    "Data Preprocessing: Label encoding, scaling, and feature engineering.\n",
    "Handling Imbalance: Using SMOTE for oversampling the minority class.\n",
    "Modeling and Hyperparameter Tuning: Using Optuna to tune the hyperparameters of models like XGBoost, LightGBM, RandomForest, and Logistic Regression.\n",
    "Evaluation: Automatically selecting the best model with the highest AUC score.\n",
    "Explainability: Using SHAP to interpret the model.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for categorical variables\n",
    "label_enc = LabelEncoder()\n",
    "df['risk_tolerance'] = label_enc.fit_transform(df['risk_tolerance'])\n",
    "df['investment_experience'] = label_enc.fit_transform(df['investment_experience'])\n",
    "df['liquidity_needs'] = label_enc.fit_transform(df['liquidity_needs'])\n",
    "df['platform'] = label_enc.fit_transform(df['platform'])\n",
    "df['time_horizon'] = label_enc.fit_transform(df['time_horizon'])\n",
    "\n",
    "# Scaling the numerical features\n",
    "scaler = StandardScaler()\n",
    "df[['time_spent', 'first_deposit_amount']] = scaler.fit_transform(df[['time_spent', 'first_deposit_amount']])\n",
    "\n",
    "# Feature Engineering: Additional features\n",
    "df['deposit_per_time'] = df['first_deposit_amount'] / (df['time_spent'] + 1)  # Avoid division by zero\n",
    "df['is_high_risk'] = (df['risk_tolerance'] == 0).astype(int)  # Assuming 0 = high risk\n",
    "\n",
    "# Prepare X and y\n",
    "X = df.drop(columns=['user_id', 'churn_flag'])\n",
    "y = df['churn_flag']\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Model Definition and Objective Function for Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we define the models: XGBoost, LightGBM, RandomForest, and Logistic Regression with L1/L2 regularization.\n",
    "# Optuna will optimize each model's hyperparameters.\n",
    "\n",
    "def objective(trial):\n",
    "    model_type = trial.suggest_categorical('model_type', ['xgboost', 'lightgbm', 'randomforest', 'logistic'])\n",
    "\n",
    "    if model_type == 'xgboost':\n",
    "        model = xgb.XGBClassifier(\n",
    "            max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "            learning_rate=trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "            subsample=trial.suggest_uniform('subsample', 0.6, 1.0)\n",
    "        )\n",
    "\n",
    "    elif model_type == 'lightgbm':\n",
    "        model = lgb.LGBMClassifier(\n",
    "            num_leaves=trial.suggest_int('num_leaves', 20, 150),\n",
    "            learning_rate=trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "            feature_fraction=trial.suggest_uniform('feature_fraction', 0.6, 1.0)\n",
    "        )\n",
    "\n",
    "    elif model_type == 'randomforest':\n",
    "        model = RandomForestClassifier(\n",
    "            max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "            max_features=trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
    "        )\n",
    "\n",
    "    elif model_type == 'logistic':\n",
    "        model = LogisticRegression(\n",
    "            penalty=trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "            C=trial.suggest_loguniform('C', 0.01, 10),\n",
    "            solver='liblinear'\n",
    "        )\n",
    "\n",
    "    # Train-Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Running Optuna for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna for hyperparameter tuning\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best trial and parameters\n",
    "print(f\"Best Trial: {study.best_trial}\")\n",
    "print(f\"Best Parameters: {study.best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Final Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Optuna identifies the best model and parameters, we will retrain the model and evaluate it on the test set\n",
    "# with detailed metrics.\n",
    "# Train the final model based on best parameters\n",
    "best_params = study.best_params\n",
    "if best_params['model_type'] == 'xgboost':\n",
    "    best_model = xgb.XGBClassifier(**best_params)\n",
    "elif best_params['model_type'] == 'lightgbm':\n",
    "    best_model = lgb.LGBMClassifier(**best_params)\n",
    "elif best_params['model_type'] == 'randomforest':\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "else:\n",
    "    best_model = LogisticRegression(**best_params)\n",
    "\n",
    "# Train on the full resampled data\n",
    "best_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate final model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Final Model Accuracy: {accuracy}')\n",
    "print(f'Final Model ROC AUC: {roc_auc}')\n",
    "print(conf_matrix)\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Model Explainability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP for feature importance\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of time spent\n",
    "sns.histplot(df['time_spent'], kde=True)\n",
    "plt.title('Distribution of Time Spent')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of first deposit amount by churn flag\n",
    "sns.boxplot(x='churn_flag', y='first_deposit_amount', data=df)\n",
    "plt.title('First Deposit by Churn Flag')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Model Explainability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot SHAP summary\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Numerical Features:\n",
    "sns.histplot(df['time_spent'], kde=True)\n",
    "plt.title('Distribution of Time Spent in App')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x='churn_flag', y='first_deposit_amount', data=df)\n",
    "plt.title('First Deposit Amount by Churn Flag')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features\n",
    "sns.countplot(x='platform', hue='churn_flag', data=df)\n",
    "plt.title('Churn by Platform')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
