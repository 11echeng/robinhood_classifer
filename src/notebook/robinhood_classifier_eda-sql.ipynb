{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation and Handling\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "\n",
    "# DB Credentials\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Handling Imbalanced Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Model Lifecycle Management\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Distributed Computing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier as SparkRFClassifier\n",
    "\n",
    "# Model Interpretability\n",
    "import shap\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Automated Feature Engineering\n",
    "import featuretools as ft\n",
    "\n",
    "# Get the current working directory\n",
    "# Get current and parent directories\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "\n",
    "# Add parent directory to sys.path\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Custom Modules\n",
    "from fetch_data_hook import fetch_sql_code, fetch_sql_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>close_equity</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-18</td>\n",
       "      <td>1786.5060</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-19</td>\n",
       "      <td>1775.8472</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>1789.8100</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-21</td>\n",
       "      <td>1778.0800</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-24</td>\n",
       "      <td>1801.6200</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119153</th>\n",
       "      <td>2017-07-11</td>\n",
       "      <td>2170.4100</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119154</th>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>2213.3100</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119155</th>\n",
       "      <td>2017-07-13</td>\n",
       "      <td>1959.2284</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119156</th>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>2000.4340</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119157</th>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>1987.0940</td>\n",
       "      <td>7ee3940c717a3066db7db0919e4e4326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1119158 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp  close_equity                           user_id\n",
       "0       2017-07-18     1786.5060  7ee3940c717a3066db7db0919e4e4326\n",
       "1       2017-07-19     1775.8472  7ee3940c717a3066db7db0919e4e4326\n",
       "2       2017-07-20     1789.8100  7ee3940c717a3066db7db0919e4e4326\n",
       "3       2017-07-21     1778.0800  7ee3940c717a3066db7db0919e4e4326\n",
       "4       2017-07-24     1801.6200  7ee3940c717a3066db7db0919e4e4326\n",
       "...            ...           ...                               ...\n",
       "1119153 2017-07-11     2170.4100  7ee3940c717a3066db7db0919e4e4326\n",
       "1119154 2017-07-12     2213.3100  7ee3940c717a3066db7db0919e4e4326\n",
       "1119155 2017-07-13     1959.2284  7ee3940c717a3066db7db0919e4e4326\n",
       "1119156 2017-07-14     2000.4340  7ee3940c717a3066db7db0919e4e4326\n",
       "1119157 2017-07-17     1987.0940  7ee3940c717a3066db7db0919e4e4326\n",
       "\n",
       "[1119158 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_sql = 'merged_data'\n",
    "equity_value_data = 'equity_value_data'\n",
    "chatgpt_sql_4o = 'chatGPT-model-4o'\n",
    "chatgpt_sql_o1 = 'chatGPT-model-o1'\n",
    "chatgpt_sql_o1_q1='chatGPT-model-o1-Q1'\n",
    "df = fetch_sql_file(equity_value_data)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
