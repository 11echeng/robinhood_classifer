{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cheng\\Workspace\\robinhood_classifer\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation and Handling\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "\n",
    "# DB Credentials\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Handling Imbalanced Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Model Lifecycle Management\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Distributed Computing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier as SparkRFClassifier\n",
    "\n",
    "# Model Interpretability\n",
    "import shap\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Automated Feature Engineering\n",
    "import featuretools as ft\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "BASE_DIR = '../SQL/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_data = pd.read_csv(r'../../data/equity_value_data.csv', parse_dates=['timestamp'])\n",
    "features_data =  pd.read_csv(r'../../data/features_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk_tolerance                   object\n",
       "investment_experience            object\n",
       "liquidity_needs                  object\n",
       "platform                         object\n",
       "time_spent                      float64\n",
       "instrument_type_first_traded     object\n",
       "first_deposit_amount            float64\n",
       "time_horizon                     object\n",
       "user_id                          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp       datetime64[ns, UTC]\n",
       "close_equity                float64\n",
       "user_id                      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equity_data['timestamp'] = pd.to_datetime(equity_data['timestamp'])\n",
    "equity_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp       datetime64[ns]\n",
       "close_equity           float64\n",
       "user_id                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equity_data['timestamp'] = equity_data['timestamp'].dt.tz_localize(None)\n",
    "equity_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate the Complete Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the min and max dates from the equity data\n",
    "min_date = equity_data['timestamp'].min()\n",
    "max_date = equity_data['timestamp'].max()\n",
    "\n",
    "# Create a date range of all calendar dates\n",
    "all_dates = pd.date_range(start=min_date, end=max_date, freq='D')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Identify Market Open and Closed Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine market open days (dates with any data)\n",
    "market_open_days = equity_data['timestamp'].dt.date.unique()\n",
    "\n",
    "# Create a DataFrame for all dates with market status\n",
    "calendar_df = pd.DataFrame({'date': all_dates})\n",
    "calendar_df['market_status'] = calendar_df['date'].dt.date.apply(\n",
    "    lambda x: 'open' if x in market_open_days else 'closed'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare User-Date Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all users\n",
    "user_ids = equity_data['user_id'].unique()\n",
    "\n",
    "# Create a MultiIndex with all users and all dates\n",
    "user_date_index = pd.MultiIndex.from_product(\n",
    "    [user_ids, all_dates], names=['user_id', 'date']\n",
    ")\n",
    "\n",
    "# Create a DataFrame with this index\n",
    "user_date_df = pd.DataFrame(index=user_date_index).reset_index()\n",
    "\n",
    "# Merge with the calendar DataFrame to get market status\n",
    "user_date_df = user_date_df.merge(calendar_df, on='date', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Merge Equity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Data types before conversion:\")\n",
    "# print(\"user_date_df['date'] dtype:\", user_date_df['date'].dtype)\n",
    "# print(\"equity_data['date'] dtype:\", equity_data['date'].dtype)\n",
    "\n",
    "# equity_data['date'] = pd.to_datetime(equity_data['date'])\n",
    "# equity_data.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare equity data by setting the index\n",
    "equity_data['date'] = equity_data['timestamp'].dt.date\n",
    "equity_data['date'] = pd.to_datetime(equity_data['date'])\n",
    "\n",
    "equity_data = equity_data[['user_id', 'date', 'close_equity']]\n",
    "\n",
    "# Merge user-date DataFrame with equity data\n",
    "user_date_df = user_date_df.merge(\n",
    "    equity_data, on=['user_id', 'date'], how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Determine Equity State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the equity state\n",
    "def determine_state(row):\n",
    "    if pd.notnull(row['close_equity']):\n",
    "        return 'equity_ge_10'\n",
    "    elif row['market_status'] == 'open':\n",
    "        return 'equity_lt_10'\n",
    "    else:\n",
    "        return None  # Will fill this later\n",
    "\n",
    "user_date_df['state'] = user_date_df.apply(determine_state, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Carry Forward the Last Known State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame\n",
    "user_date_df = user_date_df.sort_values(['user_id', 'date'])\n",
    "\n",
    "# Group by user and fill forward the state\n",
    "user_date_df['state'] = user_date_df.groupby('user_id')['state'].ffill()\n",
    "\n",
    "# For any remaining missing states at the beginning, fill with 'equity_ge_10'\n",
    "user_date_df['state'] = user_date_df['state'].fillna('equity_ge_10')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Identify Consecutive Periods of Equity < $10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure the DataFrame is sorted by 'user_id' and 'date'\n",
    "# user_date_df = user_date_df.sort_values(['user_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# # Identify consecutive sequences\n",
    "# user_date_df['grp'] = (\n",
    "#     user_date_df.groupby('user_id')['below_10']\n",
    "#     .apply(lambda x: (x != x.shift()).cumsum())\n",
    "#     .reset_index(level=0, drop=True)\n",
    "# )\n",
    "# # Filter only the periods where equity was below $10\n",
    "# # below_10_df = user_date_df[user_date_df['below_10']]\n",
    "# # Option 2\n",
    "# # # Ensure the DataFrame is sorted\n",
    "# # user_date_df = user_date_df.sort_values(['user_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# # # Identify changes in 'below_10' and compute cumulative sum\n",
    "# # user_date_df['grp'] = user_date_df.groupby('user_id')['below_10'].transform(\n",
    "# #     lambda x: (x != x.shift()).cumsum()\n",
    "# # )\n",
    "# # Option 3\n",
    "# # # Ensure the DataFrame is sorted\n",
    "# # user_date_df = user_date_df.sort_values(['user_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# # # Identify consecutive sequences\n",
    "# # user_date_df['grp'] = (\n",
    "# #     (user_date_df['below_10'] != user_date_df.groupby('user_id')['below_10'].shift())\n",
    "# #     .astype(int)\n",
    "# #     .groupby(user_date_df['user_id'])\n",
    "# #     .cumsum()\n",
    "# # )\n",
    "\n",
    "# # Final Update\n",
    "# Flag days where equity is less than $10\n",
    "user_date_df['below_10'] = user_date_df['state'] == 'equity_lt_10'\n",
    "\n",
    "# Ensure the DataFrame is sorted\n",
    "user_date_df = user_date_df.sort_values(['user_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Identify consecutive sequences without using apply()\n",
    "user_date_df['grp'] = (\n",
    "    (user_date_df['below_10'] != user_date_df.groupby('user_id')['below_10'].shift())\n",
    "    .fillna(1)  # Handle NaN values resulting from the shift\n",
    "    .astype(int)\n",
    "    .groupby(user_date_df['user_id'])\n",
    "    .cumsum()\n",
    ")\n",
    "\n",
    "# Filter only the periods where equity was below $10\n",
    "below_10_df = user_date_df[user_date_df['below_10']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Calculate Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each sequence\n",
    "sequence_lengths = (\n",
    "    below_10_df.groupby(['user_id', 'grp'])\n",
    "    .agg(start_date=('date', 'min'), end_date=('date', 'max'), num_days=('date', 'count'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the number of calendar days in each sequence\n",
    "sequence_lengths['num_calendar_days'] = (\n",
    "    sequence_lengths['end_date'] - sequence_lengths['start_date']\n",
    ").dt.days + 1  # Add 1 to include both start and end dates\n",
    "# # Calculate the length of each sequence\n",
    "# sequence_lengths = (\n",
    "#     user_date_df[user_date_df['below_10']]\n",
    "#     .groupby(['user_id', 'grp'])\n",
    "#     .agg(\n",
    "#         start_date=('date', 'min'),\n",
    "#         end_date=('date', 'max'),\n",
    "#         num_days=('date', 'count')\n",
    "#     )\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# # Calculate the number of calendar days\n",
    "# sequence_lengths['num_calendar_days'] = (\n",
    "#     sequence_lengths['end_date'] - sequence_lengths['start_date']\n",
    "# ).dt.days + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Identify Users Who Have Churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify sequences where num_calendar_days >= 28\n",
    "churned_sequences = sequence_lengths[sequence_lengths['num_calendar_days'] >= 28]\n",
    "\n",
    "# Get the list of churned users\n",
    "churned_users = churned_sequences['user_id'].unique()\n",
    "\n",
    "# # Identify sequences where num_calendar_days >= 28\n",
    "# churned_sequences = sequence_lengths[sequence_lengths['num_calendar_days'] >= 28]\n",
    "\n",
    "# # Get the list of churned users\n",
    "# churned_users = churned_sequences['user_id'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Calculate the Percentage of Users Who Have Churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of users who have churned: 45.38%\n"
     ]
    }
   ],
   "source": [
    "total_users = len(user_ids)\n",
    "num_churned_users = len(churned_users)\n",
    "percentage_churned = (num_churned_users / total_users) * 100\n",
    "\n",
    "print(f\"Percentage of users who have churned: {percentage_churned:.2f}%\")\n",
    "# # Ensure the DataFrame is sorted\n",
    "# user_date_df = user_date_df.sort_values(['user_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# # Identify consecutive sequences\n",
    "# user_date_df['grp'] = (\n",
    "#     (user_date_df['below_10'] != user_date_df.groupby('user_id')['below_10'].shift())\n",
    "#     .astype(int)\n",
    "#     .groupby(user_date_df['user_id'])\n",
    "#     .cumsum()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
